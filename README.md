# My-algorithms
These are all the algorithms that i am learning from the book 'Introduction to algorithms' and 
the nptel series 'Design and analysis of algorithms' by Madhavan Mukund from NPTEL

Afterward I have added some preprocessing models and ML Models that helps in streamlining the process of kaggle competitions.

Here are some sources to help me:
The main source is:
https://www.coursera.org/learn/competitive-data-science/


Preprocessing:
https://scikit-learn.org/stable/modules/preprocessing.html
https://sebastianraschka.com/Articles/2014_about_feature_scaling.html
https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/
https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering


Validation:
Just try to mimic as much as possible how the organisers have done the split between train and test data.
http://scikit-learn.org/stable/modules/cross_validation.html
http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/


Text Based Preprocessing:
https://scikit-learn.org/stable/modules/feature_extraction.html
https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/
https://www.tensorflow.org/tutorials/word2vec
https://rare-technologies.com/word2vec-tutorial/
http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/
https://taylorwhitten.github.io/blog/word2vec
http://www.nltk.org/
https://github.com/sloria/TextBlob


Tree Based Models:
https://scikit-learn.org/stable/modules/ensemble.html
https://xgboost.readthedocs.io/en/latest/
https://lightgbm.readthedocs.io/en/latest/
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html


Linear Models:
https://scikit-learn.org/stable/modules/linear_model.html


Neural Networks:
https://keras.io/guides/


Image Based:
https://keras.io/applications/
https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11
https://www.tensorflow.org/tutorials/image_retraining
https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html


Data Leakages:
https://www.kaggle.com/olegtrott/the-perfect-score-script
https://www.kaggle.com/docs/competitions#leakage
https://www.kaggle.com/dansbecker/data-leakage


Metrics:
https://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references


Ranking:
http://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf
https://wellecks.wordpress.com/2015/01/15/learning-to-rank-overview
https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf
https://sourceforge.net/p/lemur/wiki/RankLib/


Clustering:
http://nlp.uned.es/docs/amigo2007a.pdf



Optimization:
http://scikit-learn.org/stable/modules/grid_search.html
http://fastml.com/optimizing-hyperparams-with-hyperopt/
https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/


For starters:
https://github.com/Far0n/kaggletils
https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/




